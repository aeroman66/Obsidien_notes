# 贝尔曼最优性原理
一个最优策略拥有以下性质：不管初始状态和初始决策是什么，剩下的决策一定构成最优策略。
这体现一种面向未来的思想：不管当下的决策做的好不好，以当下状态作为初始状态时，后面的决策一定都是最优决策。
这也是动态的基本理念。
## 贝尔曼方程推导
### 马尔可夫决策过程
我们会比较注重将这些概念与控制中的概念做映射。
我们已经在概率课上学习过了马尔可夫过程。我们可以将一个控制过程也看作一个马尔可夫过程。每个时刻之间的状态转移就是马尔可夫状态的转移，而我们输入的控制量就是我们选择的动作。
	在这个例子中，状态就是我们的速度和位置，我们选择的控制量也即动作，是无人机的加速度。
![[控制马尔可夫.png]]
### 概念定义
- 马尔可夫周期(自己瞎逼逼的)
我想定义一个完整的马尔可夫周期，这在我们的公式推导中有助于理解。
$$
s_t,a_t,r_t,s_{t+1}
$$
我们有初始状态 $s_t$，然后选择一个动作 $a_t$，获取一个回报 $r_t$，最后到达新状态 $s_{t+1}$
- Episode
一个 episode 是指所有拥有相同起点和终点的马尔可夫链构成的集合。
![[episode.png]]
- 状态转移矩阵
课上学的。
- 奖励 $R_t$
从一种状态转移到另一种状态所获得的回报。事实上在控制过程中，如果我们采用了这种动作，并且到达了新的状态后，我们获得的代价函数的值就可以看作这个奖励的值。尽管奖励通常意味着我们想要越多越好，但是实际上我们想要最小化代价函数。
- 累计回报 $G_t$
在一整个马尔可夫链中获得的累积回报：
$$
G_t =  \sum_{k=0}^N \gamma ^ k R_{t+k+1}
$$
其中 $\lambda$ 是一个衰减系数，因为我们觉得离我们越远的回报越不重要，所以我们可以选一个 $]0,1[$ 的值。
- 状态函数 $V(s)$
$$
V(s) = E(G_t|S = s_t,T = s')
$$
是从 s 到 s' 的所有马尔可夫链的累积回报的期望，一个平均值。
![[累计回报value.png]]
- 状态-价值函数 $Q(s,a)$
在当前状态 s ，采取了动作 a 后，改点对应的状态函数值。该函数的得出方式我们会在 **策略** 部分中说明。
#### 计算 V 值
我们希望计算某一特定策略下个点对应的 V 值，有了这个 V 值，我们就可以做动态规划了（应该吧）。
V 值计算有很多种方法，这里我们介绍一种解方程的方法。
我们会发现，当前状态的 V 值可以由下一状态的 V 值推导得出：
$$
V(s) = R_s + \gamma\sum_{s'\in S}P(s'|s)V(s')
$$
这个式子可以用矩阵简洁地表示出来：
$$
v = R + \gamma P v
$$
$$
\begin{bmatrix}
v(1) \\
\vdots \\
v(n)
\end{bmatrix} =
\begin{bmatrix}
R_1 \\
\vdots \\
R_n
\end{bmatrix}
+
\gamma
\begin{bmatrix}
P_{11} & \cdots & P_{1n} \\
\vdots \\
P_{11} & \cdots & P_{nn}
\end{bmatrix}
\begin{bmatrix}
v(1) \\
\vdots \\
v(n)
\end{bmatrix}
$$
于是解出：
$$
v = (I - \gamma P)^{-1}R
$$
个人理解，这里等号两边的 v 之所以是一样的，是因为这个 v 是经过迭代后稳定的值，所以相等。
## 策略
我们人为选择的当前时刻选取某一特定动作的概率，并且我们通过这个动作转移到下一个状态。
![[贝尔曼方程9 1.png]]
在这种情况下，从当前状态转移到下一状态的概率，变为某种**全概率公式**计算
	之所以转移到另一种状态有很多种动作供我们选择，我们可以想象这种情形：方程数小于我们的控制量数，我们可能有无穷多组的控制量去达成我们的控制目标
$$
P_{\pi}(s'|s) = \sum_{a\in A}\pi(a|s)P(s'|a,s)
$$
这样，我们可以重写我们的 Value Function，给它加上我们的策略：
$$
\begin{align}
V_{\pi}(s) &= R^{\pi}_s + \gamma\sum_{s'\in S}P_{\pi}(s'|s)V_{\pi}(s') \\
&= R^{\pi}_s + \gamma\sum_{s'\in S}\pi(a|s)P(s'|a,s)V_{\pi}(s') \\
&= \sum_{a\in A}\pi(a|s)(R^a_s + \gamma\sum_{s'\in S}P(s'|a,s)V_{\pi}(s')) \\
R^{\pi}_s &= \sum_{a\in A}\pi(a|s)R^a_s
\end{align}
$$
我们将后面括号内的东西就定义作 Action-Value Function:
$$
\begin{align}
Q_{\pi}(s,a) &= R^a_s + \gamma\sum_{s'\in S}P(s'|a,s)V_{\pi}(s') \\
V_{\pi}(s) &= \sum_{a\in A}\pi(a|s)Q_{\pi}(s,a)
\end{align}
$$
这两条就是最普通的 Bellman Equations。总的来说，Value Function 是 Action-Value Function 关于action 的期望。
## 最优值函数
$$
V^* (s) = \min V_{\pi}(s)
$$
$$
Q^*(s,a) = \min Q_{\pi}(s,a)
$$
我们寻找最优策略的思路其实是简单的，那就是寻找使值函数最小的动作，令他的概率为 1：
$$
\pi'(a|s) = 
\begin{cases}
1 & a = \arg\max_a Q^{\pi}(s,\hat{a}) \\
0 & otherwise
\end{cases}
$$
所以我们有：
$$
V_*(s) = \max Q_* (s,a)
$$
$$
\begin{align}
Q_* (s,a) &= R^a_s + \gamma \sum_{s'\in S} P(s'|a,s)V_*(s') \\
&= R^a_s + \gamma \sum_{s'\in S} P(s'|a,s)Q_* (s',a')
\end{align}
$$