牛顿法严格来说不是一种最优化方法，因为它是一种寻找函数零点的方法。这里我们利用最优化的充要条件，来寻找梯度函数的零点来寻找我们的最小值。
# Méthode de Newton
牛顿法因为是一种二阶下降方法，所以下降速度会比梯度下降法这种一阶下降法快很多。但是因为要计算海森矩阵的逆，所以单步计算时长会长很多。
Soit $f:\mathbb{R}^{n}\rightarrow \mathbb{R}$ , 我们将这个函数在 $x_{k}$ 处用泰勒公式二次展开：
$$
f(x) = f(x_{k}) + <\nabla f(x_{k}),x - x_{k}> + \frac{1}{2}<H_{f}(x_{k})\cdot(x - x_{k}),x - x_{k}> + \| x - x_{k} \|^{2}\epsilon (x - x_{k})
$$
我们将这看作一个二次型，因此将其类比做二次函数，可以知道取最小值时 x 的取值：
$$
x = x_{k} - H_{f}(x_{k})^{-1}\cdot \nabla f(x_{k})
$$
这个式子还有另一层意思，那就是取 $f$ 的梯度函数，$x$ 的取值是 $\nabla f(x_{k})$ 切线与横轴的交点。这是我们熟悉的牛顿法找零点的方法。
所谓的牛顿法最优化，就是以下的迭代关系：
$$
x_{k+1} = x_{k} - H_{f}(x_{k})^{-1}\cdot \nabla f(x_{k})
$$
## Convergence globale
牛顿法虽然收敛很快，但这是在我们的初始点离最小值比较近的前提下。如果初始值选得太远了，那么牛顿法有可能不会收敛。
因此，我们为牛顿法引入一系列步长：
$$
x_{k+1} = x_{k} - \delta_{k}\cdot H_{f}(x_{k})^{-1}\cdot \nabla f(x_{k})
$$
这个步长可以由之前梯度下降里的方法确定，牛顿法在这里只确定下降方向。