前面我们都是从矩阵的角度去理解线性代数的。而现在，来到线性映射，事实上这是另一个理解线性代数的方式。我们来到了另一个起点上。
线性代数真正让人感兴趣的部分是线性映射,也称**线性变换**。
# 线性映射
从 V 到 W 的线性映射是具有下列性质的函数 $T:U\rightarrow W$，记作$\mathcal{L}(V,W)$。
	T : Transformation
$$
\forall u,v \in V,T(u+v) = T(u)+T(v)
$$
$$
\forall \lambda \in F, v \in V, T(\lambda v) = \lambda(Tv)
$$
**Remarque**：
1. 线性映射永远将0映射为0.
## 线性映射代数运算
- 加法
$$
(S+T)(v) = Sv + Tv
$$
- 标量乘法
$$
(\lambda T)(v) = \lambda (Tv)
$$
由以上两条可得$\mathcal{L}(V,W)$ 是一个向量空间。

- 乘积
线性映射的乘积通常为两个函数的复合。线性映射的乘积有一些代数性质。
	注意没有交换性！！
1. 结合性
2. 单位元
3. 分配性

## 零空间（核空间）和值域
这是两个与每个线性映射紧密联系的子空间。每个线性映射对应的矩阵也有这个概念。
	主要注意他们都是子空间
- 核空间
$Ker(U)$ 由被映为 0 的向量构成。
$$
Ker (U) = \{v \in V;T(v) = 0\}, U \in \mathcal{L}(V, W)
$$
1. 核空间是 V 的子空间。
2. Injective $\Leftrightarrow Ker(U)$  = \{0\}
- 值域
$Im(U)$ 是一个函数的输出集。
$$
Im(U) = \{T(v);v \in V\}
$$
## 定理
- 线性映射基本定理
对于任意线性映射 $\mathcal{L}(V,W)$ 来说
$$
\dim V = \dim Ker(T) + \dim Im(T)
$$
1. $\dim V > \dim W$ ，则线性映射T不可能是单射。
2. $\dim V < \dim W$ ，则线性映射不可能是满射。
**Exemple**：
用线性映射描述齐次线性方程组是否有非零解问题。（值得好好思考）
结论：
1.  变量数量大于方程数量时，齐次线性方程组必有非零解。 
2. 方程数大于变量数时，必有情况使得相应的非齐次线性方程组无解。

# 线性映射对应的矩阵
一个线性映射就对应了一个矩阵，矩阵是用坐标来描述一个线性映射。也就是：
	数学是一门数学化和符号化的语言，所以我觉得这类表达式还是蛮重要的。
$$
T(v) = Av
$$

## 原理
如果我们有了一个线性映射，那么需要关于这个映射的什么信息才允许我们将它对应的矩阵确定下来呢？
事实上，我们知道向量空间中的每个向量都可以由该空间中的基通过唯一的线性组合得到。这给了我们启发，知道 V 空间中所有基向量经过线性变换后的结果，就可以让我们唯一确定一个线性映射。
但是在此之前，我们还需要引入坐标的概念。可以发现本节之前的推导都没有借助任何坐标系。而矩阵却源于坐标。事实上，平时我们所写的向量，其内的各个元素都是对应基向量前的系数，只是我们一直没意识到这个事实。而在线性变换的过程中，将会很考验我们的这种思维。如：
$$
x = 
\begin{bmatrix}
3 \\
4 \\
5
\end{bmatrix}
=
3
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix}
+
4
\begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix}
+
5
\begin{bmatrix}
0 \\
0 \\
1
\end{bmatrix}
$$
假设 $T \in \mathcal{L}(V,W)$, 并且 $v_{1},\dots,v_{n}$, V 的 base，$w_{1},\dots,w_{m}$ W 的 base ，即：
$$
v = c_{1}v_{1} + c_{2}v_{2} + \cdots + c_{n}v_{n}
$$
$$
\begin{align}
T(v) &= c_{1}T(v_{1}) + c_{2}T(v_{2}) + \cdots + c_{m}T(v_{m}) \\
&= d_{1}w_{1} + d_{2}w_{2} + \cdots + d_{m}w_{m}
\end{align}
$$
## 求法
1. 找到 V，W 空间内的一组基 $v_{1},\dots,v_{n}$，$w_{1},\dots,w_{m}$
2. 求解：
$$
T(v_{1}) = a_{11}w_{1} + a_{21}w_{2} + \cdots + a_{m1}w_{m}
$$
$$
T(v_{2}) = a_{12}w_{1} + a_{22}w_{2} + \cdots + a_{m2}w_{m}
$$
我们所求的矩阵为：
$$
\begin{bmatrix}
a_{11} & a_{12} &  \cdots & a_{1n} \\
\vdots &&& \vdots \\
a_{m1} & a_{m2} &  \cdots & a_{mn}
\end{bmatrix}
$$
矩阵有很多可以和线性变换做类比的性质：
- 得出特定尺寸的矩阵也是一个向量空间。
- 矩阵乘法：只有第一个矩阵的列数等于第二个矩阵的行数时，我们才能定义这两个矩阵的乘积。
1. 不满足交换律
2. 满足结合律
3. 满足分配律
- 线性映射复合的矩阵，相当于矩阵乘法：
$$
T \in \mathcal{L}(U,V),S \in \mathcal{L}(V,W), \mathcal{M}(ST) = \mathcal{M}(S)\mathcal{M}(T)
$$
- 矩阵运算
1. $\mathcal{M}(T)_{.,k} = \mathcal{M}(Tv_{k})$ 是矩阵的第 k 列
2. 线性映射的作用类似于矩阵乘：$\mathcal{M}(Tv) = \mathcal{M}(T)\mathcal{M}(v)$
## Exemple
- 二维平面投影到一三象限的平分线上。我们分别选取两组基来完成这个操作
1. 与平分线共线和垂直于平分线的向量作为输入输出空间的基：
$$
P = \frac{a^{T}a}{aa^{T}}
$$
得到：
$$
P(v_{1}) = v_{1} = 1 * v_{1} + 0*v_{2};P(v_{2}) = 0 = 0 * v_{1} + 0*v_{2}
$$
所以：
$$
\mathcal{M}(P) =
\begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix}
$$
2. 取常用的 x,y 轴作为输入输出空间的基：
$$
P = \frac{a^{T}a}{aa^{T}}
$$
得到：
$$
P(v_{1}) = 
\begin{bmatrix}
\frac{1}{2} \\
\frac{1}{2}
\end{bmatrix}
= \frac{1}{2} * v_{1} + \frac{1}{2} * v_{2};
P(v_{2}) = 
\begin{bmatrix}
\frac{1}{2} \\
\frac{1}{2}
\end{bmatrix}
= \frac{1}{2} * v_{1} + \frac{1}{2} * v_{2}
$$
$$
\mathcal{M}(P) =
\begin{bmatrix}
\frac{1}{2} & \frac{1}{2} \\
\frac{1}{2} & \frac{1}{2}
\end{bmatrix}
$$
我们发现，基选的不同，相同线性映射得到的矩阵也不同。
- 求导
事实上，求导本身是一种线性映射，所以允许我们靠很少的几个函数的求导规则来对大量的函数进行求导操作。
$$
T = \frac{d}{dx}
$$
设我们的输入向量和对应的输出向量为：
$$
c_{1} + c_{2}x + c_{3}x^{2}, c_{2} + c_{3}x
$$
从中我们可以知道我们的输入空间和输出空间的基分别是：
$$
\{ 1,x,x^{2} \},\{ 1,x \}
$$
得到线性变换对应的矩阵：
$$
\begin{cases}
T(1) = 0 \\
T(x) = 1 * 1 + 0 * x \\
T(x^{2}) = 0 * 1 + 2 * x
\end{cases}
$$
$$
\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 2
\end{bmatrix}
$$
求导过程可表示为：
$$
\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 2
\end{bmatrix}
\begin{bmatrix}
c_{1} \\
c_{2} \\
c_{3}
\end{bmatrix}
=
\begin{bmatrix}
c_{2} \\
2c_{3}
\end{bmatrix}
$$